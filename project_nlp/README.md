# Project NLP: Web APIs & NLP Part 1    

_**Authors:** Zhan Yu_  
 


## Table of Contents
- [Problem Statement](#Problem-Statement)  
- [Executive Summary](#Executive-Summary)
- [Conclusion](#Conclusion)
- [Recommendations and limitation](#Recommendations-and-limitation)
- [References](#References)


## Problem Statement    
   
Sephora currently does not have a **chatbot** which is a computer program that simulates and processes human conversation (in this cast, text messages), allowing customers to interact with digital devices as if they were communicating with a real person.  
Sephora website would like to initiate a "chatbots" in order to improve their customer experience. But human power is expensive so the main purpose of this project is that, with Natural Language Processing (NLP) we could build a model which is able to identify which questions are needed to be responded by customer service employees and which could simply be generated by computer.  
The model in this project is based on two subreddit: [Sephora](https://www.reddit.com/r/Sephora/) and [Makeup](https://www.reddit.com/r/Makeup/). Sephora subreddit has 14.5k members and it discusses anything Sephora-related such as makeup and skincare advice. Makeup subreddit has 146k members and it talks about makeup tips and advices. 
In this project, we are going to use Linear Regression, Naive Bayes and Bagging Models and evaluate using accuracy.  

## Executive Summary

We broke down our process into two parts (two notebooks):  

### Part 1:   
**Data Gathering**   
First we made a function using [pushshift.io Reddit API](https://github.com/pushshift/api) which provides enhanced functionality and search capabilities for searching Reddit comments and submissions, to gather post data from two subreddit: [Sephora](https://www.reddit.com/r/Sephora/) and [Makeup](https://www.reddit.com/r/Makeup/). Sephora subreddit has 14.5k members and Makeup subreddit has 146k members so for Sephora subreddit, we use 3 times period (`times = 15`) of Makeup subreddit (`times = 5`) to balance the two classes. 

**Data Cleaning** We pulled in the two subreddit data that was scraped and combine them into one dataframe. First we dropped rows with missing values and columns we don not need. Next we changed 'subreddit' column to binary class: `{'Makeup': 0, 'Sephora': 1}` and removed website links and `\n`. We finished our cleaning process by defining a function called `words_only` to convert a semi-raw text to a string of words.  

### Part 2:  
**Feature Engineering**  
We tested Stemming, Lemmentizing and without Stemming/Lemmentizing for the `text` column whitin the models we made, compared with Accuracy, and we got a conclusion that Lemmentizing is the best option.


**Modeling**  
First we established baseline model. In this project we are going to build a model to predict if a post is from "Makeup" or "Sephora". So this is a classification problem and the baseline model is predicting majority class and So our baseline is 0.582881.  
Then we built Linear Regression, Naive Bayes and Bagging Models with two vectorizers `CountVectorizer` and `TfidfVectorizer`.   
For Logistic Regression Models, we are usd `Pineline` to put `CountVectorizer` and `TfidfVectorizer`, and `LogisticRegression` model together with `GridSearchCV` to find best parameters.   
In Naive Bayes Models part, for `CountVectorizer` we used model `MultinomialNB`and for `TfidfVectorizer` we used model `GaussianNB`.   
For Bagging Models, we used `BaggingClassifierfor` with both `CountVectorizer` and `TfidfVectorizer`.  

**Data Visualization**  
We used `CountVectorizer` and `TfidfVectorizer` with two words `ngram_range=(2, 2)` which it is easier for us to see the most popular words in two subreddit.  

**Model Evaluation**  
Naive Bayes Models for `CountVectorizer` with `MultinomialNB` is our best model, according to the Accuracy.    
With our baseline score 0.582881:  

| Model | Accuracy of Traning Data | Accuracy of Testing Data |  
|---|---|---|  
| Logistic Regression with CountVectorizer | 0.8952 | 0.8979 |  
| Logistic Regression with TfidfVectorize | 0.9003 | 0.9081 |  
| MultinomialNB with CountVectorizer | 0.9013 | **0.9018** |  
| GaussianNB with TfidfVectorizer |0.9505 |0.8445 |  
| BaggingClassifierfor with CountVectorizer | 0.9845 | 0.8767 |  
| BaggingClassifierfor with TfidfVectorizer | 0.9875 | 0.8767 |  

Naive Bayes Models for `CountVectorizer` with `MultinomialNB` is our best model, according to the Accuracy. From condusion matrix, we can see that for all Makeup subreddit predictions, 93.94% we predict right; for all Sephora subreddit predictions, 84.93% we predict right.  

## Conclusion  

With Natural Language Processing (NLP) we could build a model which is able to identify which post are from Makeup subreddit(needed to be responded by customer service employees) and which are from Sephora subreddit (answers could simply be generated by computer) with a fairly high prediction accuracy.  
The model solves the problem with our best model - Naive Bayes for CountVectorizer with MultinomialNB which can have the accuracy as 90%.  

## Recommendations and limitation   
Sephora website would use "chatbots" in order to improve their customer experience, save time and labor power and it is a better data gathering and storing method.  

**Limitations**  
- Some Ad blocker could block Chatbots.  
- Texting preference (e.g. Languages): This model is only built base on English language.  
- Miscommunications: Texting is sometimes not as efficient as conversations.  

## References     
- [Chatbot-Wikipedia](https://en.wikipedia.org/wiki/Chatbot)   
- [Sephora-Wikipedia](https://en.wikipedia.org/wiki/Sephora)  
- [Pushshift Reddit API Documentation](https://github.com/pushshift/api)
