{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Web APIs & NLP Part 1    \n",
    "\n",
    "_**Authors:** Zhan Yu_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement    \n",
    "   \n",
    "Sephora currently does not have a **chatbot** which is a computer program that simulates and processes human conversation (in this cast, text messages), allowing customers to interact with digital devices as if they were communicating with a real person.  \n",
    "Sephora website would like to initiate a \"chatbots\" in order to improve their customer experience. But human power is expensive so the main purpose of this project is that, with Natural Language Processing (NLP) we could build a model which is able to identify which questions are needed to be responded by customer service employees and which could simply be generated by computer.  \n",
    "The model in this project is based on two subreddit: [Sephora](https://www.reddit.com/r/Sephora/) and [Makeup](https://www.reddit.com/r/Makeup/). Sephora subreddit has 14.5k members and it discusses anything Sephora-related such as makeup and skincare advice. Makeup subreddit has 146k members and it talks about makeup tips and advices. \n",
    "In this project, we are going to use Linear Regression, Naive Bayes and Bagging Models and evaluate using accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "We broke down our process into two parts (two notebooks):  \n",
    "\n",
    "### Part 1:   \n",
    "**Data Gathering**   \n",
    "First we made a function using [pushshift.io Reddit API](https://github.com/pushshift/api) which provides enhanced functionality and search capabilities for searching Reddit comments and submissions, to gather post data from two subreddit: [Sephora](https://www.reddit.com/r/Sephora/) and [Makeup](https://www.reddit.com/r/Makeup/). Sephora subreddit has 14.5k members and Makeup subreddit has 146k members so for Sephora subreddit, we use 3 times period (`times = 15`) of Makeup subreddit (`times = 5`) to balance the two classes. \n",
    "\n",
    "**Data Cleaning** We pulled in the two subreddit data that was scraped and combine them into one dataframe. First we dropped rows with missing values and columns we don not need. Next we changed 'subreddit' column to binary class: `{'Makeup': 0, 'Sephora': 1}` and removed website links and `\\n`. We finished our cleaning process by defining a function called `words_only` to convert a semi-raw text to a string of words.  \n",
    "\n",
    "### Part 2:  \n",
    "**Feature Engineering**  \n",
    "We tested Stemming, Lemmentizing and without Stemming/Lemmentizing for the `text` column whitin the models we made, compared with Accuracy, and we got a conclusion that Lemmentizing is the best option.\n",
    "\n",
    "\n",
    "**Modeling**  \n",
    "First we established baseline model. In this project we are going to build a model to predict if a post is from \"Makeup\" or \"Sephora\". So this is a classification problem and the baseline model is predicting majority class and So our baseline is 0.582881.  \n",
    "Then we built Linear Regression, Naive Bayes and Bagging Models with two vectorizers `CountVectorizer` and `TfidfVectorizer`.   \n",
    "For Logistic Regression Models, we are usd `Pineline` to put `CountVectorizer` and `TfidfVectorizer`, and `LogisticRegression` model together with `GridSearchCV` to find best parameters.   \n",
    "In Naive Bayes Models part, for `CountVectorizer` we used model `MultinomialNB`and for `TfidfVectorizer` we used model `GaussianNB`.   \n",
    "For Bagging Models, we used `BaggingClassifierfor` with both `CountVectorizer` and `TfidfVectorizer`.  \n",
    "\n",
    "**Data Visualization**  \n",
    "We used `CountVectorizer` and `TfidfVectorizer` with two words `ngram_range=(2, 2)` which it is easier for us to see the most popular words in two subreddit.  \n",
    "\n",
    "**Model Evaluation**  \n",
    "Naive Bayes Models for `CountVectorizer` with `MultinomialNB` is our best model, according to the Accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Problem Statement](#Problem-Statement)  \n",
    "- [Executive Summary](#Executive-Summary)\n",
    "- [Gathering Data](#Gathering-Data)\n",
    "- [Data Cleaning](#Data-Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries   \n",
    "\n",
    "All the libraries will be imported here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup  \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are going to use [Pushshift's](https://github.com/pushshift/api) API for collecting posts from two subreddits [Sephora](https://www.reddit.com/r/Sephora/) and [Makeup](https://www.reddit.com/r/Makeup/) from [reddit](https://www.reddit.com/).   \n",
    "First, we are going to define a function which could help us easily get posts data from website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pushshift(subreddit, \n",
    "                    kind='submission', \n",
    "                    skip=30, \n",
    "                    times=5, \n",
    "                    subfield = ['title', 'selftext', 'subreddit', \n",
    "                                'created_utc', 'author', 'num_comments', \n",
    "                                'score', 'is_self'],\n",
    "                    comfields = ['body', 'score', 'created_utc']):\n",
    "    # Setting URL:\n",
    "    stem = \"https://api.pushshift.io/reddit/search/{}/?subreddit={}&size=500\".format(kind, subreddit)\n",
    "    mylist = []\n",
    "    for x in range(1, times + 1):\n",
    "        URL = \"{}&after={}d\".format(stem, skip * x)\n",
    "        # The URL will show the time periods\n",
    "        print(URL)\n",
    "        \n",
    "        # Gathering the data:\n",
    "        response = requests.get(URL)                     # library \"requests\"\n",
    "        assert response.status_code == 200               # check the status code\n",
    "        mine = response.json()['data']                   # get all the data\n",
    "        df = pd.DataFrame.from_dict(mine)                # make a dataframe\n",
    "        mylist.append(df)                                # the list of dataframes we made\n",
    "        # Set time as every 2 seconds\n",
    "        time.sleep(2)\n",
    "    \n",
    "    full = pd.concat(mylist, sort=False)                 # put all dataframes together\n",
    "    if kind == \"submission\":\n",
    "        full = full[subfield]                            # keep only columns we want\n",
    "        full = full.drop_duplicates()                    # drop all duplicated rows\n",
    "        full = full.loc[full['is_self'] == True]         # only submission is a self post\n",
    "    \n",
    "    # Transforming the dates:\n",
    "    def get_date(created):\n",
    "        return dt.date.fromtimestamp(created)\n",
    "    _timestamp = full[\"created_utc\"].apply(get_date)\n",
    "    full['timestamp'] = _timestamp                        \n",
    "    print(full.shape)\n",
    "    return full "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary  \n",
    "\n",
    "| Feature | Description | Type |\n",
    "| ------ | ------ | ------ |\n",
    "| title | Name of a post | String |\n",
    "| selftext | Content of a post | String |\n",
    "| subreddit | Specific subreddit | String |\n",
    "| timestamp | Posting date | String |\n",
    "| is_self | Restrict results based on if submission is a self post | Boolean |\n",
    "| created_utc | Restrict results based on the epoch value given or range of values | Integer |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since [Makeup](https://www.reddit.com/r/Makeup/) is a much bigger and more popular topic, we choose `times=5` and for [Sephora](https://www.reddit.com/r/Sephora/) we choose `times=15` to balance the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=30d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=60d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=90d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=120d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=150d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=180d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=210d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=240d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=270d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=300d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=330d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=360d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=390d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=420d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Sephora&size=500&after=450d\n",
      "(1769, 6)\n"
     ]
    }
   ],
   "source": [
    "df_sephora = query_pushshift(subreddit='Sephora', \n",
    "                      times=15, \n",
    "                      subfield = ['title', 'selftext', 'subreddit', 'created_utc', 'is_self'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Makeup&size=500&after=30d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Makeup&size=500&after=60d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Makeup&size=500&after=90d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Makeup&size=500&after=120d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=Makeup&size=500&after=150d\n",
      "(2481, 6)\n"
     ]
    }
   ],
   "source": [
    "df_makeup = query_pushshift(subreddit='Makeup', \n",
    "                      times=5, \n",
    "                      subfield = ['title', 'selftext', 'subreddit', 'created_utc', 'is_self'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we gathered data of two subreddits, we are going to some data cleaning:  \n",
    "First, we combine the two dataframes together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4250, 6)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_sephora, df_makeup], ignore_index=True)\n",
    "\n",
    "# Check the shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'title' and 'selftext' together and create a new column 'text'\n",
    "df['text'] = df['title'] + ' ' + df['selftext']\n",
    "\n",
    "# Drop the columns we don't need:\n",
    "df = df.drop(columns=['title', 'selftext', 'created_utc', 'is_self'])\n",
    "\n",
    "# Change 'subreddit' column to binary class:\n",
    "df['subreddit'] = df['subreddit'].map({'Makeup': 0, 'Sephora': 1})\n",
    "\n",
    "# Removed website links and '\\n'\n",
    "df['text'] = df['text'].str.replace('http\\S+|www.\\S+', ' ', case=False).str.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit    0\n",
       "timestamp    0\n",
       "text         6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values:\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit   timestamp text\n",
       "1837          0  2020-01-02  NaN\n",
       "1851          0  2020-01-02  NaN\n",
       "1882          0  2020-01-03  NaN\n",
       "1915          0  2020-01-04  NaN\n",
       "1917          0  2020-01-04  NaN\n",
       "1921          0  2020-01-04  NaN"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text'].isnull()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 6 rows and reset index\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values again:\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to define a function called `words_only` to convert a semi-raw text to a string of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_only(raw):\n",
    "\n",
    "    # 1. Remove non-letters.\n",
    "    letters_only = re.sub('[^a-zA-Z]', ' ', raw)\n",
    "    \n",
    "    # 2. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 3. Join all the stopwords as a string with \" \", remove \"'\" from the stopwords and split it as a list.\n",
    "    stops = \" \".join(stopwords.words('english')).replace(\"'\", \"\").split()\n",
    "    \n",
    "    # 4. Remove stopwords.\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 5. Join the words back into one string separated by space and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>final predictions beauty insider program happy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>love go nearly points really hope going fixed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>points disappeared bought things december st c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>sephora birthday perks days redeem sephora bir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>bite beauty relaunching went sephora bite beau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit   timestamp                                               text\n",
       "0          1  2019-12-31  final predictions beauty insider program happy...\n",
       "1          1  2020-01-01  love go nearly points really hope going fixed ...\n",
       "2          1  2020-01-01  points disappeared bought things december st c...\n",
       "3          1  2020-01-01  sephora birthday perks days redeem sephora bir...\n",
       "4          1  2020-01-01  bite beauty relaunching went sephora bite beau..."
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(words_only)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values again:\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/meaningful_words.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
